{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/swathisl170499/Retrieval_Augmented-Generation-RAG-Solution/blob/main/VectorDB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EU-TGXO3tE9k",
        "outputId": "4949c58f-1e49-4358-9970-827f63f6cff1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.11.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.11/dist-packages (0.19.0)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.6.15)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.14.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.73.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.2.1)\n",
            "Requirement already satisfied: nltk>=3.9 in /usr/local/lib/python3.11/dist-packages (from textblob) (3.9.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (1.5.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install requests tqdm faiss-cpu transformers tensorflow sentence-transformers textblob gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_Zx-cM6tKky",
        "outputId": "7b794ed1-9958-4b03-fef2-d1f3273c6f94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "88121KB [00:06, 13357.59KB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting dataset...\n",
            "Dataset downloaded and extracted.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import requests\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Directory to store downloaded and extracted data\n",
        "DATA_DIR = Path(\"./mimic_textbooks\")\n",
        "\n",
        "# Download and extract the dataset zip file\n",
        "def download_and_extract_zip(url, extract_to=DATA_DIR):\n",
        "    # Ensure the directory exists\n",
        "    extract_to.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Download the zip file\n",
        "    zip_path = extract_to / \"textbooks.zip\"\n",
        "    print(\"Downloading dataset...\")\n",
        "    response = requests.get(url, stream=True)\n",
        "    with open(zip_path, \"wb\") as file:\n",
        "        for chunk in tqdm(response.iter_content(chunk_size=1024), unit='KB'):\n",
        "            if chunk:\n",
        "                file.write(chunk)\n",
        "\n",
        "    # Extract the zip file\n",
        "    print(\"Extracting dataset...\")\n",
        "    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
        "        zip_ref.extractall(extract_to)\n",
        "    print(\"Dataset downloaded and extracted.\")\n",
        "\n",
        "# Download and extract textbooks\n",
        "dataset_url = \"https://www.dropbox.com/scl/fi/54p9kkx5n93bffyx08eba/textbooks.zip?rlkey=2y2c5x8y0uncnddichn9cmd7n&st=m290nmkk&dl=1\"\n",
        "download_and_extract_zip(dataset_url)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0e9bbpzvtNFX",
        "outputId": "5e8f87a5-a87e-41c6-fb6a-6759da040d86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total document chunks created: 60061\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "from gensim.utils import simple_preprocess\n",
        "from textblob import TextBlob\n",
        "\n",
        "# Load text files\n",
        "def load_text_files(directory):\n",
        "    texts = []\n",
        "    for file_path in Path(directory).glob(\"*.txt\"):\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "            texts.append(file.read())\n",
        "    return texts\n",
        "\n",
        "# Cleaning and preprocessing function\n",
        "def clean_and_tokenize(text):\n",
        "    # Basic regex cleaning\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n",
        "    text = text.lower()  # Lowercase all text\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)  # Remove special characters\n",
        "\n",
        "    # Tokenize with gensim\n",
        "    tokens = simple_preprocess(text)\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Spell correction\n",
        "def correct_spelling(text):\n",
        "    return str(TextBlob(text).correct())\n",
        "\n",
        "# Chunk text into fixed-size chunks\n",
        "def chunk_text(text, chunk_size=200):\n",
        "    words = text.split()\n",
        "    return [' '.join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size)]\n",
        "\n",
        "# Load, clean, correct, and chunk documents\n",
        "documents = load_text_files(DATA_DIR / \"textbooks/en\")\n",
        "cleaned_documents = [clean_and_tokenize(doc) for doc in documents]\n",
        "# corrected_documents = [correct_spelling(doc) for doc in cleaned_documents]\n",
        "chunked_documents = []\n",
        "for doc in cleaned_documents:\n",
        "    chunked_documents.extend(chunk_text(doc))\n",
        "\n",
        "print(f\"Total document chunks created: {len(chunked_documents)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435,
          "referenced_widgets": [
            "a84758ab871447eb9746ee1d61b4549f",
            "e6fbc1c40f6e41abba0a4340be1c84ef",
            "330f0e2c2a284fbe999b275abd2f4f5c",
            "ca266a10f82b434b92b74d123e4fa676",
            "567647909c704d6b86b2045c621c5141",
            "f1a3a0cb0af24df99f73df0724954562",
            "75f5c0f24a344a09b49bdde8ae7757ae",
            "3e6ef5bf5a124b409b22f5b8144f16a4",
            "ffe2415c51c4464282a422aa8a5a577a",
            "570ee52f537c4b28ae16348d7e9b863c",
            "dbfe3403d7314623936a0e1c891fe78a",
            "6e75af5fa2ee4831ae61722e88cc6cf9",
            "f5a2a84e48e44c918bd448809eabfe7a",
            "ab1508d7bbbc4860b27c1be3e5185b0b",
            "fe275d1e320c4427a90689dfd4a5015d",
            "d4e04257ba3541fb8df64a8077c1744d",
            "ed634a534e254c519a328e8631d232bc",
            "628b26d2d6f347b7b422403d62bd533e",
            "c7b4cedc2e1a456d9c2fa80259dcfc04",
            "94e837964462483a8cbac414c6f0a091",
            "d82d5d74cd764ab783bbc1ff54f9c9ef",
            "8c60c06c08a74128b1147c25d411ad10",
            "c000c630e6244e24be54e16c905d569b",
            "30df6c2ea72945d3897b156131e7946d",
            "3de6a8ddc1a14fae907b61912c2ef9c7",
            "30b21ccf95394e479d56c280fd898f93",
            "44f1c56d356e41c284d26a4cebe04547",
            "ff53c16329884e2a9c1cf8d6cfd4e58b",
            "7f2d94e65d3a41079297d792715caaa8",
            "8b33cd41d0e7455f8c4a0557552939a8",
            "6dd73fad209b416781e3780f68561eb3",
            "d795e12126f8409db9052dee67f4cfe7",
            "a9e04fc2bf254198bb94f93e1347de89",
            "46fd96e419114e1da7d18b63431c29f8",
            "fcf703c0ab0344f1a784928bc6fa8817",
            "d416d7d76799425781037aeed0468700",
            "d1006e04f20a49cf9f94c1f530303d4e",
            "32ef4aee0ff84340b38382fe84300fee",
            "737c9e458d8b449b9fe99fb313025cc7",
            "3664b7cb6f2644f19b3f48e7becf6b42",
            "41c463da9f32490588359f185e66f3a6",
            "ca0386c8a4e041188f574a7f0451e7bc",
            "63c36937fc7f4b88a389e7572cdd3643",
            "4733179133e04443862b121c272f8884",
            "76ad9bda733b40f19ce7e9188b5fc736",
            "6f1c54e5a7414ad289126ffb4ef2fdbb",
            "55707b13554f49af87113ebdbfb453c9",
            "02b470afb0dc45949204a57aa957d90c",
            "f38282fb0a524a7db539c4a4297fc2f3",
            "b0dc855f60a043978cefc440dca1e919",
            "8b8a70318fdb49e4b84711d08ef6eec6",
            "0bec56aa9512405791349551ec71f0cf",
            "6df38fa2e04143b1a9f124ed83303560",
            "308ab8cae70848d8a8d569ae019ed0b2",
            "b06fd8d1cba548fdb997eea8d21de727",
            "17ea3335be2d401db13f7220fb690a8b",
            "8dc270e104d242b5bacc84be2ec1a294",
            "b380103306194f469626fca386b404a6",
            "3af5e9b133af4e3d98929a892de98f0e",
            "bccae8f93bfc4877bf3e12278c202f80",
            "a8237a21743843b18b51fe57af44f7ba",
            "9028453c1cd74d49baafec34a8013849",
            "bf3acc1c701b4becb16dde61d2441f64",
            "963ca35e89c14c3f9ef646946c51789a",
            "6e6465244b4840399f30361380fe4b2e",
            "afe9aff09da445e68ee95f90237f7651"
          ]
        },
        "id": "p7qLJ9NYtUnV",
        "outputId": "3fa8b39f-5de4-4ee3-9c13-427fcf0f9962"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Available devices: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a84758ab871447eb9746ee1d61b4549f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6e75af5fa2ee4831ae61722e88cc6cf9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c000c630e6244e24be54e16c905d569b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "46fd96e419114e1da7d18b63431c29f8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "76ad9bda733b40f19ce7e9188b5fc736",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "17ea3335be2d401db13f7220fb690a8b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['embeddings.position_ids']\n",
            "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFBertModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated embeddings for 60061 document chunks.\n"
          ]
        }
      ],
      "source": [
        "from transformers import TFAutoModel, AutoTokenizer\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Verify that TensorFlow detects the GPU\n",
        "print(\"Available devices:\", tf.config.list_physical_devices('GPU'))\n",
        "\n",
        "# Load the model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "model = TFAutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "# Function to generate embeddings for all chunks in a batch\n",
        "def get_embeddings_in_batch(texts, batch_size=16):\n",
        "    all_embeddings = []\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch_texts = texts[i:i + batch_size]\n",
        "\n",
        "        # Tokenize the batch of texts\n",
        "        inputs = tokenizer(batch_texts, return_tensors=\"tf\", truncation=True, padding=True, max_length=512)\n",
        "\n",
        "        # Generate embeddings on the GPU\n",
        "        outputs = model(inputs).last_hidden_state  # [batch_size, sequence_length, hidden_size]\n",
        "        batch_embeddings = tf.reduce_mean(outputs, axis=1).numpy()  # Mean pooling\n",
        "\n",
        "        # Append batch embeddings to the list\n",
        "        all_embeddings.extend(batch_embeddings)\n",
        "\n",
        "    return np.array(all_embeddings)\n",
        "\n",
        "# Generate embeddings for all document chunks in batches\n",
        "embeddings = get_embeddings_in_batch(chunked_documents, batch_size=128)\n",
        "print(f\"Generated embeddings for {len(embeddings)} document chunks.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gFJHoP_0tX5F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a196f5a4-1df4-4dc6-e7a4-681b3572d349"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total embeddings indexed: 60061\n"
          ]
        }
      ],
      "source": [
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "# Define the dimension of embeddings\n",
        "dimension = 384  # Embedding size from MiniLM model\n",
        "index = faiss.IndexFlatL2(dimension)\n",
        "\n",
        "# Convert embeddings to NumPy array for FAISS\n",
        "embedding_matrix = np.array([embedding.flatten() for embedding in embeddings]).astype('float32')\n",
        "\n",
        "# Add embeddings to FAISS index\n",
        "index.add(embedding_matrix)\n",
        "print(f\"Total embeddings indexed: {index.ntotal}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tb7gGdMetZX_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d6ef259-aac1-412e-91c7-ed97800fd961"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top similar document chunks:\n",
            "down to six principal mechanisms failure of the pump in the most common situation the cardiac muscle contracts weakly and the chambers cannot empty systolic dysfunction in some cases the muscle cannot relax sufficiently to permit ventricular filling resulting in diastolic dysfunction obstruction to flow lesions that prevent valve opening eg calcific aortic valve stenosis or cause increased ventricular chamber pressures eg systemic hypertension or aortic coarctation can overwork the myocardium which has to pump against the obstruction regurgitant flow valve pathology that allows backward flow of blood results in increased volume workload and may overwhelm the pumping capacity of the affected chambers shunted flow defects congenital or acquired that divert blood inappropriately from one chamber to another or from one vessel to another lead to pressure and volume overloads disorders of cardiac conduction uncoordinated cardiac impulses or blocked conduction pathways can cause arrhythmias that slow contractions or prevent effective pumping altogether rupture of the heart or major vessel loss of circulatory continuity eg gunshot wound through the thoracic aorta may lead to massive blood loss hypotensive shock and death heart failure often referred to as congestive heart failure chf is the common end point for many forms of cardiac\n",
            "disease and typically is progressive condition with poor prognosis in the united states alone over million individuals are affected resulting in well over million annually and financial burden in excess of billion roughly one half of patients die within years of receiving diagnosis of chf and in deaths in the united states include heart failure as contributory cause chf occurs when the heart cannot generate sufficient output to meet the metabolic demands of the tissues or can only do so at filling pressures in minority of cases heart failure is consequence of greatly increased tissue demands as in hyperthyroidism or decreased oxygen carrying capacity as in anemia highoutput failure the onset of chf is sometimes abrupt as in the setting of large myocardial infarct or acute valve dysfunction in most cases however chf develops gradually and insidiously owing to the cumulative effects of chronic work overload or progressive loss of myocardium heart failure may result from systolic or diastolic dysfunction systolic dysfunction results from inadequate myocardial contractile function usually as consequence of ischemic heart disease or hypertension diastolic dysfunction refers to an inability of the heart to adequately relax and fill which may be consequence of massive left ventricular hypertrophy\n",
            "myocardial fibrosis amyloid deposition or constrictive pericarditis approximately one half of chf cases are attributable to diastolic dysfunction with greater frequency seen in older adults diabetic patients and women heart failure may also be caused by valve dysfunction eg due to endocarditis or may occur following rapid increases in blood volume or blood pressure even if the heart is normal when the failing heart can no longer efficiently pump blood there is an increase in enddiastolic ventricular volumes increased enddiastolic pressures and elevated venous pressures thus inadequate cardiac output called forward failureis almost always accompanied by increased congestion of the venous circulationthat is backward failure although the root problem in chf typically is deficient cardiac function virtually every other organ is eventually affected by some combination of forward and backward failure the cardiovascular system attempts to compensate for reduced myocardial contractility or increased hemodynamic burden through several homeostatic mechanisms the frankstarling mechanism increased enddiastolic filling volumes dilate the heart and cause increased cardiac myofiber stretching these lengthened fibers contract more forcibly thereby increasing cardiac output if the dilated ventricle is able to maintain cardiac output by this means the patient is said to be in compensated heart failure however ventricular\n",
            "unexpected death due to cardiac disease occurs without symptoms or hour after symptoms arise usually due to fatal ventricular arrhythmia most common etiology is acute ischemia of patients have preexisting severe atherosclerosis less common causes include mitral valve prolapse cardiomyopathy and cocaine abuse poor myocardial function due to chronic ischemic damage with or without infarction progresses to congestive heart failure chf basic principles pump failure divided into rightand leftsided failure ii leftsided heart failure causes include ischemia hypertension dilated cardiomyopathy myocardial infarction and restrictive cardiomyopathy clinical features are due to decreased forward perfusion and pulmonary congestion pulmonary congestion leads to pulmonary edema results in dyspnea paroxysmal nocturnal dyspnea due to increased venous return when lying flat orthopnea and crackles ii small congested capillaries may burst leading to intraalveolar hemorrhage marked by macrophages heartfailure cells fig decreased flow to kidneys leads to activation of system fluid retention exacerbates chf mainstay of treatment is ace inhibitor iii rightsided heart failure most commonly due to leftsided heart failure other important causes include lefttoright shunt and chronic lung disease cor pulmonale clinical features are due to congestion painful with characteristic nutmeg liver fig may lead to cardiac cirrhosis dependent pitting edema due to increased\n",
            "have dynamic obstruction to the left ventricular outflow by the anterior leaflet of the mitral valve reduced cardiac output and secondary increase in pulmonary venous pressure cause exertional dyspnea with harsh systolic ejection murmur combination of massive hypertrophy high left ventricular pressures and compromised intramural arteries frequently leads to myocardial ischemia with angina even in the absence of concomitant cad major clinical problems include atrial fibrillation with mural thrombus formation ventricular fibrillation leading to sudden cardiac death infectious endocarditis of the mitral valve and chf most patients symptoms are improved by therapy that promotes ventricular relaxation partial surgical excision or controlled alcoholinduced infarction of septal muscle also can relieve the outflow tract obstruction as mentioned earlier hcm is an important cause of sudden cardiac death in almost one third of cases of sudden cardiac death in athletes younger than years of age the underlying cause is hcm restrictive cardiomyopathy is characterized by primary decrease in ventricular compliance resulting in impaired ventricular filling during diastole simply put the wall is stiffer this form of cardiomyopathy may be idiopathic or may be associated with systemic diseases that affect the myocardium for example radiation fibrosis amyloidosis sarcoidosis or products of inborn errors of\n"
          ]
        }
      ],
      "source": [
        "# Example query for testing\n",
        "\n",
        "def get_embedding(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"tf\", truncation=True, padding=True)\n",
        "    outputs = model(inputs).last_hidden_state\n",
        "    return tf.reduce_mean(outputs, axis=1).numpy()  # Average pooling for sentence embedding\n",
        "\n",
        "\n",
        "\n",
        "query_text = \"What are causes of heart failure?\"\n",
        "query_embedding = get_embedding(query_text)\n",
        "query_embedding = np.array(query_embedding).reshape(1, -1).astype('float32')\n",
        "\n",
        "# Search FAISS for the most similar documents\n",
        "k = 5  # Number of closest documents to retrieve\n",
        "distances, indices = index.search(query_embedding, k)\n",
        "\n",
        "# Retrieve and print the most similar chunks\n",
        "print(\"Top similar document chunks:\")\n",
        "for idx in indices[0]:\n",
        "    print(chunked_documents[idx])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YeBUoKYNxQ51"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}